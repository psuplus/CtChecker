\section{Evaluation}
\subsection{Implementation}

Achieving the improved precision of the analysis is done primarily by
handling LLVM's pointer instruction differently than other instructions. The
pointer instruction in the IR is the GEP instruction. This instruction
is used when computing an address from a base pointer. For arrays, for example
it has the index of the element and the bounds of the array if they are known.
For structures, the instruction contains the structure type which is being
addressed and the index of the field in the structure that is being referenced.
The baseline analysis lacked the ability to consider the index available in
these instructions. The points-to analysis had type and offset
information for the targets of the pointers. Improving the analysis was achieved
by i) creating the appropriate number of constraint elements for each node in
the points-to graph, ii) converting the field index from the GEP instruction to
a byte offset, and iii) constraining the correct elements based on
the GEP instruction and data type information.

\subsubsection{Constraint Element Generation}
When a pointer value is encountered for the first time, a node is created for it
in the points-to graph and a set of constraint elements are created for that
data structure. The points-to analysis was left unmodified, so the focus will be
on the semantics of how constraint elements were changed. In the baseline, only
one constraint element represented any target of a pointer. In the improved
analysis, type information from the points-to analysis was used to generate the
appropriate number of constraint elements. In a stack allocated array, one
constraint element was created for each element in the array. For structures and
classes, each field is located at some offset away from the base address of the
structure and a constraint element was created for each field and mapped to
associated byte offset. This change enables the precision of the baseline to be
improved by correctly selecting the corresponding constraint element based on
the IR.

For any GEP instruction, constraint elements are generated using the type
information from the points-to analysis when available. Each constraint element
is created by iterating through the type information of the node from the
points-to analysis. For each type there is an associated byte offset, using this
offset and the size as reported by LLVM, a constraint element is created with
the corresponding starting byte offset and the ending byte offset based on the
width of the field. It is necessary to account for the width and start location
of each field because there may be padding between fields of a structure.
Padding between fields may be present in the type information because of an
unused field in code. The points-to analysis will not have type information on
unused fields so it is important to account for these gaps. Another reason to
account for the padding is due to structures which are not aligned with each
field following sequentially after the previous field. When the type information
is not available one constraint element is created for the whole node so at
worst the improved analysis will be equivalent to that of the baseline.

Figure \ref{alg:generateconselem} shows how the points-to analysis is
leveraged to create constraint elements for the proper byte offsets and widths.
Let \texttt{s} be the LLVM representation of the structure type referenced by
the GEP instruction, and \texttt{node} be the node in the memory graph provided
by the points-to analysis. The algorithm works by retrieving the graph in which
the node resides and then retrieving the data layout as specified by the
compiler for that graph. The data layout contains information like the field
lengths of each type and the alignment within a data structure. The structure
type itself \texttt{s} does not have the alignment and padding information, only
the types of each field within the data structure. The data layout is used to
retrieve the structure layout of the structure type from the GEP instruction.
The structure layout is vital because it allows the index from the GEP
instruction to be converted to a byte offset within the data structure. The
data layout also provides the size of each type, so that the constraint element
is created at the corrects starting and ending offset.


\begin{figure}[h!]
  \lstinputlisting{examples/createConsElem.cpp}
  \caption{Creating constraint elements for each field in a type}
  \label{alg:generateconselem}
\end{figure}

The elements created using this strategy enable the analysis to be more precise.
GEP instructions can also be used to index arrays, so that is handled by
computing the number of elements that array may hold. For stack arrays that
number is known, but for heap arrays the number is variable. For stack arrays
each, since the number of elements is known, one constraint element is created
per array element. For heap arrays, one constraint element is created for the
entire array.

Similarly, if type information is unavailable then a conservative approach is
used. One constraint element is created for the entire data structure. When
the information is available, the analysis is able to be improved by
constraining the correct constraint element for each instruction. When that
information is unavailable, the improved analysis is unable to be more precise
than the baseline analysis.

\subsubsection{Constraining Operands}
Each time an instruction where information flow exists, constraints are
generated between the operands of that instruction. The idea of this is simple,
given a set of constraint elements select the ones which are used in the
operation and constrain them as necessary. The baseline analysis handled
individual variables in a precise manner as long as the variable was not a
structure or array. The baseline analysis had no way of identifying the offset
from a GEP instruction, and even if it did, the mapping between a memory node to
a constraint element was one-to-one. In the previous section, the mapping from
memory node to constraint element was made to be one-to-many. As one node may
represent multiple fields which together make the structure or array. It is now
necessary to pick the correct element from this one-to-many mapping when
generating the constraint for each instruction. Again, this is done by analyzing
the GEP instruction.

The last operand of a GEP instruction is the index of the field within the data
structure. Each constraint element for a memory node is created for a byte
offset range, so it is necessary to correctly calculate the byte offset from the
field index provided by the GEP instruction. This is very similar to the
strategy used to generate the constraints at the correct byte locations. Given
the structure type, field index and node from the points-to analysis, the offset
is calculated as shown in figure \ref{alg:findoffset}. Given the node, the
data layout and associated structure layout can be found and then the index can
be converted to a byte offset.

\begin{figure}[h!]
\lstinputlisting{examples/findoffset.cpp}
\caption{Calculating byte offset from index}
\label{alg:findoffset}
\end{figure}

If the GEP instruction is in reference to an array access, then the offset is
just calculated using the size of the element type and the index. If no type
information exists for the node, only a single constraint element would exist
for the node, so that is the constraint which is selected to be constrained.

\subsubsection{Missing Type Information}

The type information used to generate the constraints came from the points-to
analysis. The analysis is conservative in the type information it
provides. This means that the type information given is limited to only fields
that have been used in the source, and the type information is cleared if the
analysis can not make definitively determine the types along with their offsets.
The requirement to have a field used in the source means that often the type
information from the structure is incomplete due to unused fields or fields
which external functions may use. The points-to analysis clearing type
information removes the ability to have a field sensitive constraint at all.

In the case of missing the unused data information, the solution is to utilize
the information that the points-to analysis uses to build the type map. Given a
structure type in LLVM, the list of fields and their types are known. The
points-to analysis relies on the data layout provided in the bitcode. Constraint
elements are created by querying the data layout to identify the initial byte at
which the type starts and the width of the type to find where that type should
end. The start and end of the elements are recorded when creating the constraint
elements for that type.

Building the type information always instead of on-demand is necessary when
there are external function calls where the source is not linked. If a pointer
which holds sensitive data is a parameter for that function, then it is possible
that sensitive data may flow to other arguments or the return value. When the
field-sensitivity was implemented, some of the higher-risk results had been
removed. This happened because the pointer to sensitive data was no longer
considered sensitive as it was when offset was unused. Before, it was enough to
see if any of the arguments were sensitive, and create flows from the sensitive
data to all sinks from that function. After the field sensitivity change, the
pointer and its data are constrained separately, so when external calls happen,
the analysis creates constraints from each of the fields which are reachable
from that pointer in addition to the normal flows which are provided by the
function call.

If there is no type information at all, then the improved and baseline analysis
function the same way, there is one element that represents that type and that
one element is constrained regardless of offset. The points-to analysis denotes
unsafe type information by collapsing the node in the memory graph.
\subsection{Benchmarks}

The baseline analysis had many false positives and low-risk positives. Improving
the analysis would mean reducing the positives to only results which are
possible flaws. The benchmark results shown in table
\ref{tbl:overall-feature-benchmark} show the effects of different features on
the number of reported secret-dependent branch warnings. There are 2 features
implemented which were looked at in isolation, the white-list and the
field-sensitivity changes. Additionally, adding sources for functions which are
not defined in the file being analyzed allows for a better understanding of
information flow.

The baseline benchmark evaluated the modular exponentiation code for each
library. This benchmark was run without white-list or field sensitivity. The
source provided was that only which was necessary to compile the modular
exponentiation code. All multi-precision integer (mpi) arithmetic library components
were omitted.

The field-sensitive (FS) test is the same as the baseline benchmark but with
field-sensitivity enabled. The results from this test differ depending on the
library. For example, OpenSSL did not see any benefit from FS alone. This is due
to openSSL doing much of the math in external functions, and each external
function return value was checked in the modular exponentiation code. Since
field-sensitivity requires source to work, it makes sense that openSSL did not
see benefit in this test. The reasoning is the same for the result in mbedTLS.
Libgcrypt however does manipulate the tainted data within the source analyzed so
it did benefit from FS over the baseline benchmark.

The white-list (WL) test is the same as the baseline benchmark but only using a
white-list to remove results which are clearly false or low-risk positives from
the baselines results. The white-list allowed the libraries which did not
benefit from field-sensitivity to see a reduction in positives. Libgcrypt saw an
improvement as well but not nearly as much as the field-sensitive change.

The full source (SRC) test is the same as the baseline but with additional
source files provided. The additional sources provided were the remainder of the
multi-precision integer or big number(bn) libraries. This meant that all mpi or bn math
library calls were known functions. The important fact here is that the flows
were more direct now between parameters in functions. Without field-sensitivity
though all data-structures and arrays are treated as one element. This improved
the number of results in all the libraries which did not benefit from
field-sensitivity.

The last two tests were combining features to see if greater improvement was
possible. The white-list and field-sensitivity test was chosen since the
field-sensitivity is not able to function when external functions are called
frequently, but white-listing can be used to compensate. The last test combined
the 3 individual tests, using the baseline analysis with field-sensitivity, a
white-list and the entire mpi/bn libraries.

The full combination of improvements over the baseline is where we get to see
that adding additional source in combination with the field-sensitive changes
improved the results in OpenSSL and mbedTLS the most. Libgcrypt however did see
an increase in the number of positives, which is result based on the limitations
of the points-to analysis used. The points-to analysis, makes use of
heap-cloning to track distinct nodes which may be processed by a common source
function. The analysis also merges nodes that are found to be in the same
equivalence class. In the case of Libgcrypt, nodes that were considered distinct
in the previous benchmarks, end up aliased to the same node in the full
benchmark. This causes an inferior result to the WL/FS test.

\begin{table*}
  \centering
  % \begin{tabular}{|p{3.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}| p{2.5cm}|}
    \begin{tabular}{@{}lrrrrrrrrrrr@{}}
    %\toprule

    \input{benchmark-table.tex}

    %\bottomrule
    \end{tabular}
  \caption{Number of Warnings based on Features}
  \label{tbl:overall-feature-benchmark}
\end{table*}

\begin{table*}
  \centering
  % \begin{tabular}{|p{3.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}| p{2.5cm}|}
    \begin{tabular}{@{}lrrrrrrrrrrrr@{}}
    %\toprule

    \input{benchmark-table-new-libs.tex}

    %\bottomrule
    \end{tabular}
  \caption{Number of Warnings based on Features (New-ArgToAllReachable)}
  \label{tbl:overall-feature-benchmark-new}
\end{table*}

\iffalse
\begin{table*}
	\centering
	% \begin{tabular}{|p{3.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}| p{2.5cm}|}
		\begin{tabular}{@{}lrrrrrrrrrrr@{}}
			%\toprule
			
			\input{benchmark-table-new-libs-1.tex}
			
			%\bottomrule
		\end{tabular}
		\caption{Number of Warnings based on Features (New-1)}
		\label{tbl:overall-feature-benchmark-new-1}
	\end{table*}
\fi

\iffalse
\begin{table*}
	\centering
	% \begin{tabular}{|p{3.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}| p{2.5cm}|}
		\begin{tabular}{@{}lrrrrrrrrrrr@{}}
			%\toprule
			
			\input{benchmark-table-new-libs-2.tex}
			
			%\bottomrule
		\end{tabular}
		\caption{Number of Warnings based on Features (New-2)}
		\label{tbl:overall-feature-benchmark-new-2}
	\end{table*}
\fi

\subsection{Case Study}
%The case study serves to demonstrate the changes to the results with the
%improved analysis over the baseline. Each version of the analysis is compared by
%analyzing the modular exponentiation code used in SSL and TLS libraries.
%OpenSSL and Libgcrypt were selected due to vulnerabilities that were able to be
%found by this analysis. The mbed TLS uses the same modular exponentiation as
%OpenSSL and libgcrypt but does not provide constant time implentations. BearSSL
%is selected due to its claim to be a constant time library.

The baseline and improved analysis were compared using current software TLS and
SSL libraries. The libraries analyzed were Libgcrypt, OpenSSL, mbed TLS, and
BearSSL. The case study was done by comparing the modular exponentiation
algorithms between each of the libraries. The Libgcrypt and OpenSSL libraries
were chosen due to their wide-spread use. The mbed TLS library is built for
embedded platforms, so this was chosen to search for shortcuts that may provide
vulnerabilities. BearSSL is chosen because it claims to be a constant-time
cryptographic library.

\subsubsection{Experiments}
The parameters used to test the libraries were set such that the data alone was
set to be tainted, meaning that no other members which shared the same structure
should be treated as sensitive data. The results of the improved analysis should
eliminate many of the false-positives, and some of the low-risk results. The
number of high-risk results should not change.

The source code which implemented modular exponentiation functions were analyzed in the
following configurations:
\begin{enumerate}
\item Baseline Analysis
\item Baseline with field-sensitivity changes only
\item Baseline with white-list only
\item Baseline with full library source code
\item Baseline with whitelist and field-sensitivity
\item Baseline with field-sensitivity, whitelist and full library source code
low-risk results.
\end{enumerate}

%% \begin{table}
  %% \caption{Baseline and Improved Results - Same Input}
  %% \begin{tabular}{|l|r|r|r|r|r|r|r|r|r|r|}
    %% \hline
%% Library & Num. Branches & Baseline & Positives & False Positives & Low-Risk & Validation & Single & Looped & Controlled & Both \\
    %% \hline
%% LibGcrypt 1.8.2 &	246 &	69 &	68 &	2 &	47 &	3 &	4 &	12 &	0 &	0 \\
%% mbedTLS 2.9.0 &	1471 &	25 &	25 &	0 &	18 &	3 &	0 &	3 &	1 &	0 \\
%% BearSSL 0.5 &	141 &	36 &	10 &	0 &	10 &	0 &	0 &	0 &	0 &	0 \\
    %% \hline
%% OpenSSL 1.1.0g &  & & & & & & & & & \\
%% recp	& 498 &	1 &	24 &	0 &	11 &	11 &	0 &	2 &	0 &	0 \\
%% mont	& 498 &	6 &	34 &	0 &	14 &	16 &	0 &	2 &	2 &	0 \\
%% mont\_consttime	& 498 &	8 &	30 &	0 &	16 &	12 &	0 &	0 &	2 &	0 \\
%% mont\_word	& 498 &	11 &	22 &	0 &	5 &	13 &	1 &	1 &	0 &	2 \\
%% simple	& 498 &	0 &	22 &	0 &	12 &	8 &	0 &	2 &	0 &	0 \\
    %% \hline
  %% \end{tabular}
  %% \label{tbl:baseline_improved_same}
%% \end{table}


\subsubsection{Potential Sources of Unsoundness}
For these experiments, the results given may not be all the results possible
from the source analyzed due to underlying assumptions. Since the full source
code was not analyzed, there are function calls which are defined in files which
are not compiled and linked. One assumption made for unknown functions is that
the flow only exists between the parameters. There is a possibility for this
assumption to break down if, for example, pointers are modified to point to
something which is not a value provided in the parameter. Another potential
cause of missing results is due to only considering explicit flows. The analysis
is able to handle both explicit and implicit flow, but currently they are only
able to be run independently. The experimental results shown only consider
explicit flows.

\subsubsection{Performance}

\begin{table*}[!t]
  \centering
    \ra{1.2}
  \begin{tabular}{@{}lcrrrrcrrrr@{}}
    \toprule
     & & \multicolumn{4}{c}{Minimal Source} & &  \multicolumn{4}{c}{Full Source} \\
    \cmidrule{3-6} \cmidrule{8-11}
    Library&& Baseline & Improved & \# Branches & KSLOC && Baseline & Improved & \# Branches & KSLOC\\
    \midrule
    Libgcrypt 1.8.2 && 00:24 & 00:24 & 246 & 6.5 &&  17:42 & 19:37 & 1938 & 11.1\\
    BearSSL 0.5 && 00:21 & 00:21 & 141  & 3.6 && 00:21 & 00:27  & 141 & 3.6\\
    mbedTLS 2.9.0 && 00:07 & 00:07& 214 & 0.5 && 01:14 & 01:45  & 1471 & 1.7\\
    OpenSSL 1.1.0g && 00:27 & 00:29& 498 & 18.7 && 11:00 & 13:11 & 1880 & 28.8\\
    \bottomrule
  \end{tabular}
\caption{Baseline and Improved Analysis Run Time (mm:ss)}
\label{tbl:runtimes}
\end{table*}



The performance running times of the baseline is compared with the best
performing improved analysis in Table \ref{tbl:runtimes}. For each library, the
test was done with both the full source (including the full big number library)
and the minimal source (only the modular exponentiation code). The
tests show how difference in size of the program affects the running time. The comparison
also shows that the increase in processing time in the improved analysis is
small even for the larger libraries such as libgcrypt and OpenSSL.

\subsubsection{Result Classification}
 Of all the experiments, two of the tests were classified to understand the
 category for the positives. The first experiment classified is the baseline
 experiment. Ideally no high-risk results are eliminated when moving from the
 baseline to a higher precision result. For most of the libraries the best
 results were given when combining the full source with the analysis that
 utilized field-sensitivity and white-listing, so those results were classified
 to compare against the baseline.

 The results from the experiments are sorted into four categories:
 false-positives, error-handling, low-risk and high-risk. The categories are used to
 build a white-list to further refine the results from the analysis.

 Results which are classified as false positives are marked due to not being
 calculated from sensitive data, being marked sensitive as a result of
 imprecision. This analysis is not flow-sensitive, so results which involve
 values which are re-computed from sensitive data later in an execution are also
 considered false-positives. In the baseline analysis, false positives show up
 due to the fact that a structure or an array is treated as one entity. Fields
 within that structure or array which have not been computed from sensitive data
 are reported falsely.

 Error-handling results are branches which lead to exit or error handling code.
 Error-handling branches will not execute for valid inputs. For any valid input
 then, the result of this branch will be the same. The branch may check
 sensitive data but if the result of the branch is known to an adversary, only
 trivial information would be gained (i.e. whether or not the data is valid).

 Results which are not in the first two categories are then sorted between the
 low- and high-risk sets of results. The distinction between a low and a
 high-risk result is on how much information is contained about sensitive data
 if the outcome of a branch result is known. High-risk means that there is a
 possibility to leak multiple bits of sensitive data from either a single or
 repeated execution of the branch. Low-risk results can be identified if the
 values in the branch condition can be the same across a set of sensitive
 values. For example, many number libraries have a branch based on the length of
 a sensitive value, but that value is going to be the same for a range of
 values.

 If the analysis reports a large number of results, it can be helpful to utilize
 the classifications done to remove entries from the list. The white-list can be
 compiled by identifying the variables which propagate sensitive data but is
 considered acceptable. The white-list is often very small yet can eliminate
 many of the false-positive or low-risk results.

 The second stage of classification was done after adding a small set of
 variables to the white-list. This stage breaks down the high-risk category from the
 previous stage based on the surrounding context. The first criterion is whether
 or not the branch is within a loop. The second criterion is if the branch is
 controllable. The high-risk results can then be classified as either, single,
 repeated, controllable, or loop-controllable.

 Controllable means that the branch outcome is dependent on input from an
 untrusted source. In the case of modular exponentiation, the plain-text used in
 the cryptographic algorithms is untrusted, ignoring the effects of blinding. So
 a branch would be controllable if there was a comparison between the untrusted
 data and tainted data such as the power or modulus.

 Single branches are branches are not within a loop and are not controllable.
 Controllable branches are not within a loop. Repeated branches are within a
 loop and are not controllable. Loop-controllable branches are branches which
 are within a loop and are controllable.

\subsubsection{Experiment Results}

    In this section results are shown from the library test which had the lowest
    number of positives while not losing high-risk results. Ideally, the total
    number of positives decreases and the high-risk results are not sacrificed
    for that outcome. The looped and controllable results are the ones that are
    considered high-risk. The summary for all the libraries is given in Table
    \ref{tbl:allpositives}. After each libraries results are discussed to
    understand both the false positives and the high-risk results.

\iffalse
\begin{table*}[!t]
  \centering
  \begin{tabular}{@{}lrr|crrrcrr@{}}
    \toprule
    &  &  &&  & \multicolumn{2}{c}{Low-Risk} && \multicolumn{2}{c}{High-Risk}\\
    \cmidrule{6-7} \cmidrule{9-10}
    Library& Base & Improved&& FP& Error-Handling & Low Leakage && Looped & Controllable \\
    \midrule
    \textbf{Libgcrypt 1.8.2}& 64 & 24 &&8 & 10 & 1 && 5 & 0\\
    \textbf{mbedTLS 2.9.0}  & 40 & 25 &&2 & 13 & 3 && 3  & 1\\
    \textbf{BearSSL 0.5}    & 1 & 1   &&0 & 1 & 0  && 0  & 0\\
    \textbf{OpenSSL 1.1.0g}\\
    \hspace{0.25cm}Reciprocal              & 32 & 13 && 3    & 2   & 6 && 2 & 0\\
    \hspace{0.25cm}Montgomery              & 34 & 21 &&  3   &  5  & 9 && 2  & 2\\
    \hspace{0.25cm}Montgomery Const. Time  & 30 & 12 &&  0   &  7  & 3 && 0   & 2\\
    \hspace{0.25cm}Montgomery Word         & 22 & 14 &&  2   & 6   & 5 && 1   & 0\\
    \bottomrule
  \end{tabular}
\caption{Result Classifications: Base - Baseline Positives, Improved - WL/FS/SRC
Positives}
  \label{tbl:allpositives}
\end{table*}
\fi

\begin{table*}[!t]
	\centering
	\begin{tabular}{@{}lrr|crrrcrr@{}}
		\toprule
		&  &  &&  & \multicolumn{2}{c}{Low-Risk} && \multicolumn{2}{c}{High-Risk}\\
		\cmidrule{6-7} \cmidrule{9-10}
		Library& Base & Improved&& FP& Error-Handling & Low Leakage && Looped & Controllable \\
		\midrule
		\textbf{Libgcrypt 1.10.0}& 46 & 20 && 13 &  1  &  1 &&  5 & 0 \\
		\textbf{mbedTLS 3.2.1}  & 41 & 20 && 9 &  2  & 5  && 4   & 0 \\
		\textbf{BearSSL 0.6}    & 1 & 1   && 1 & 0  & 0   &&  0  & 0 \\
		\textbf{OpenSSL 1.1.1q}\\
		\hspace{0.25cm}Reciprocal              & 14 & 3 &&  1    &  0   & 0  &&  2 & 0 \\
		\hspace{0.25cm}Montgomery              & 35 & 15 &&  1    &  4   &  6 &&  2  & 2 \\
		\hspace{0.25cm}Montgomery Const. Time  & 29 & 13 &&  9    &  0  & 4  &&  0   & 0 \\
		\hspace{0.25cm}Montgomery Word         & 4 & 2 &&   1   &   0  & 0  &&   1  & 0 \\
		\bottomrule
	\end{tabular}
	\caption{Result Classifications: Base - Baseline Positives, Improved - WL/FS/SRC
		Positives}
	\label{tbl:allpositives}
\end{table*}


All libraries (BearSSL excluded) saw an improvement from adding
field-sensitivity, whitelist and/or additional source. The configuration which
yielded the best results was including all 3 options. BearSSL did not improve
with all 3 configuration options, but all the other libraries saw a reduction in
low-risk or false positives. Libgcrypts best configuration did not include
additional source code. Adding extra source code to the tests did not yield the
least positives out of all the benchmarks for that library.


Libgcrypt had many false-positives removed between the improved and the baseline
analysis. The false positives here were due to the multi-precision integer
structures having multiple fields containing public data. Many of the branches
flagged as vulnerable were falsely reported due to the baseline analysis not being field-sensitive.

 In the mbed TLS library had no changes when the offset was used, so the results
 here were not due to field-sensitivity issues. BearSSL does not use a structure,
 but instead just an array to represent multi-precision integers. There were no
 additional fields so field-sensitivity had no effect on the results for this
 library either. The false positives however were due to the length being
 computed from the data within the integers.

 OpenSSL showed some reduction in the number of results while maintaining all of
 the high-risk results. The number of false positives did not change, but the
 number of low-risk results were removed by adding field-sensitivity. OpenSSL
 does not modify the multi-precision integers in the files analyzed so many
 calculations and the flow between them are uncertain.

\subsubsection{Libgcrypt High-risk results}


     Libgcrypt has a multi-precision integer library with a file specifically
     for modular exponentiation. This library operates on the inputs within this
     source file, only executing external function calls for other math
     operations. Unlike mbedTLS and openSSL, the branches are based on the
     inputs instead of the return value of external functions. This is evident
     by the fact that adding field-sensitivity alone reduced the number of
     positives from the baseline. The best result for this library was when
     white-listing and field-sensitivity were used together.

     The first high-risk results comes from the MPN\_NORMALIZE call, which is a
     macro, which scans the exponent pointer and sets the size accordingly. This
     is done in a branch while directly checking the value of the sensitive
     data. Once the first non-zero element is found, esize reflects the position
     of that element. The source is shown in figure \ref{code:libgcrypt_normalize}.

\begin{figure}[h!]
\begin{lstlisting}
#define MPN_NORMALIZE(d, n)    \
    do {		                   \
      while( (n) > 0 ) {       \
        <@\textcolor{red}{if( (d)[(n)-1] )}@>          \
            break;	           \
        (n)--;	               \
      }		                     \
} while(0)
\end{lstlisting}
\caption{Libgcrypt 1.8.2 - mpi-internal.h lines 113-120}
\label{code:libgcrypt_normalize}
\end{figure}

This code is listed within the macro \texttt{MPN\_NORMALIZE} and the input
parameters are a pointer $d$ and a size $n$. The exponent data pointer is passed
as $d$ and its corresponding size $n$. The if branch is then dependent on the
sensitive data stored at the pointer. This result is interesting because it is
not a constant-time implementation.

The next four results are within the main processing loop portion of the modular
exponentiation code. The main loop of the has a precomputed set of powers and
the correct member of that set is selected based on the set bit of the exponent.
The first high-risk result from this comparison is shown in line 2 of the
abbreviated main loop code shown in figure \ref{code:libgcrypt_mainloop}.

The multi-precision integer structure represent large numbers by partitioning
each number into multiple \textit{limbs}. Each of these limbs is
processed individually in this loop. For each iteration of the loop $e$ is set
to the current limb being processed. The \textit{count\_leading\_zeros}
is called to ensure the first bit in the limb is a set bit. Along with the $e$
there is a variable \codevar{c} which tracks how many bits are to be processed in the
limb. The value of \codevar{c} changes between each iteration and is dependent on the
value of the exponent. Since, \codevar{e} is the data of one limb of the exponent
directly, and \codevar{c} is computed directly from the limbs data, these are high-risk
variables.

Within the main processing loop, the first branch is on $e$ checking if there
are no set bits in the limb $e$ (line 3, figure \ref{code:libgcrypt_mainloop}).
If this is the case, the process just moves on to the next limb. If there are
set bits, in $e$ the rest of the limb is scanned in order to compute the result.
If the outcome of this branch is determined, then the value of the entire limb
is known and this is a high-risk result.
\begin{figure}[h!]
\begin{lstlisting}
e = ep[i];
e = (e << c) << 1;
...
for(;;)
  <@\textcolor{red}{if (e == 0)}@>
    {
      j += c;
      if ( --i < 0 )
        break;

      e = ep[i];
      c = BITS_PER_MPI_LIMB;
    } else ...
\end{lstlisting}
\caption{Libgcrypt lines 609-626}
\label{code:libgcrypt_mainloop}
\end{figure}


The operations done within the else are done to select the proper precomputed
values based on the value of the limb. The variable $c$ is the position of the
first set bit within a limb.  The value of $c$ is computed by looking
at each bit from a limb in the exponent, making $c$ high-risk. The branch on
line 6 (figure \ref{code:libgcrypt_c_window}) is in the main processing loop, so it is classified as a looped high-risk result.


\begin{figure}[h!]
\begin{lstlisting}
count_leading_zeros (c0, e);
e = (e << c0);
c -= c0;
j += c0;

e0 = (e >> (BITS_PER_MPI_LIMB - W));
<@\textcolor{red}{if (c >= W)}@>
  c0 = 0;
else
  {
    if ( --i < 0 ) {
        e0 = (e >> (BITS_PER_MPI_LIMB - c));
        j += c - W;
        goto last_step;
      }
    else
      {
        c0 = c;
        e = ep[i];
        c = BITS_PER_MPI_LIMB;
        e0 |= (e >> (BITS_PER_MPI_LIMB - (W - c0)));
      }
  }
\end{lstlisting}
\caption{Libgcrypt lines 635-658}
\label{code:libgcrypt_c_window}
\end{figure}

At the end of each loop iteration, a non-constant time for-loop runs (Fig \ref{code:libgcrypt_non_const_for}). The
for-loop varies with the number of leading zeros for the exponent. The value of
$j$ depends on the \codevar{c0}, a value derived from the limb of the exponent. This
makes \codevar{j} a high-risk variable, and the for loop branch iterates $j > 0$ meaning
that the number of iterations of the loop is not constant time. In this section
of the source, there are precautions taken to target some timing channels such
as indexing into the precomp array. The conditional set makes it such that for
each value of \codevar{k}, the array value is stored. Similarly, to mask the size of the
answer stored in \codevar{base\_u}, the \codevar{base\_u\_size} is computed and set each time, but only
the bit mask allows only the true size to be set.

\begin{figure}[htpb]
\begin{lstlisting}
  <@\textcolor{red}{for (j += W - c0; j >= 0; j--)}@>
    {

      /*
        *  base_u <= precomp[e0]
        *  base_u_size <= precomp_size[e0]
        */
      base_u_size = 0;
      for (k = 0; k < (1<< (W - 1)); k++)
        {
          w.alloced = w.nlimbs = precomp_size[k];
          u.alloced = u.nlimbs = precomp_size[k];
          u.d = precomp[k];

          mpi_set_cond (&w, &u, k == e0);
          base_u_size |= ( precomp_size[k] & (0UL - (k == e0)) );
        }

      w.alloced = w.nlimbs = rsize;
      u.alloced = u.nlimbs = rsize;
      u.d = rp;
      mpi_set_cond (&w, &u, j != 0);
      base_u_size ^= ((base_u_size ^ rsize)  & (0UL - (j != 0)));

      mul_mod (xp, &xsize, rp, rsize, base_u, base_u_size,
                mp, msize, &karactx);
      tp = rp; rp = xp; xp = tp;
      rsize = xsize;
    }
\end{lstlisting}
\caption{Libgcrypt mpi-pow.c lines 667-695}
\label{code:libgcrypt_non_const_for}
\end{figure}

The while loop shown in figure \ref{code:libgcrypt_whilej} is done after the
main processing loop. The value of \codevar{j} is related to the number of
non-set bits in the last processed limb making it a high-risk variable. The
branch is the condition for a loop, so this is a looped high-risk result. This
result was covered in an attack paper, which showed that knowing the sequence of
squares and multiplies could lead to leakages of set bits in the exponent. This
attack could function by having a program run on the same machine that is
executing the vulnerable code\cite{bernstein2017sliding}. Another attack is based on this same code, which
does not require a program running on the machine, it uses a hardware technique
to measure electro-magnetic emanations from laptops\cite{genkin2015stealing}.

Results like these are important because the attacks are sophisticated, but are
based on the knowledge that information about the target information is
available in the sense to be measured. Gaining knowledge that this branch is
possibly tainted by confidential data does not guarantee a side-channel. This
result is helpful to identify code where a side-channel may exist.

\begin{figure}[htpb]
\begin{lstlisting}
<@\textcolor{red}{while (j--)}@>
  {
    mul_mod (xp, &xsize, rp, rsize, rp, rsize,
                          mp, msize, &karactx);
    tp = rp; rp = xp; xp = tp;
    rsize = xsize;
  }
\end{lstlisting}
\caption{Libgcrypt mpi-pow.c lines 702-707}
\label{code:libgcrypt_whilej}
\end{figure}


\subsubsection{mbedTLS 2.9.0}

     The big number library for mbedTLS is a single source file. For the
     baseline results, the file was modified to only include the source required
     to compile and analyze the modular exponentiation code. The least amount of
     positives was found when combining, field-sensitivity, white listing and
     including the entire big number source. The high risk results in this
     library are branches which look at a single bit in the exponent.

The mbedTLS library had no change in the number of positives with the
field-sensitivity alone. However combining field-sensitivity with the full
library source code improved results.

In the modular exponentiation code, there is a reduction of the base \codevar{a}
if it is greater than the modulus \codevar{n}. The modulus \codevar{n} is marked to be tainted
and \codevar{a} is the multi-precision integer that represents the base. The base is the
user input, and is untrusted. This branch on line 1673 of the source file (\codefile{bignum.c})
directly compares the value of \codevar{a} to the value of \codevar{n}. Although
the modulus is a public value, this result shows that branch conditions that
lead to paths which take time differences based on execution path. Here there is
an untrusted input, the plain-text represented as an integer \codevar{A}
compared directly with a value marked sensitive \codevar{N}. Depending on the
value of the untrusted data in regards to the sensitive data, the code either
does a copy operation or conducts a modulus operation.

\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
<@\textcolor{red}{if( mbedtls\_mpi\_cmp\_mpi( A, N ) >= 0 )}@>
    MBEDTLS_MPI_CHK( mbedtls_mpi_mod_mpi( &W[1], A, N ) );
else
    MBEDTLS_MPI_CHK( mbedtls_mpi_copy( &W[1], A ) );
\end{lstlisting}
\rulebelow

\caption{mbedTLS 2.9.0 - bignum.c lines 1672-1676}
\end{figure}

The 3 looped results reside in the loop which inspects the bits of the exponent.
The data for the limbs containing the exponent is located at \codevar{E$\rightarrow$p}. Each
iteration through the loop looks at one bit of the exponent directly, making
\codevar{ei} a high-risk variable in this loop. The first branch on \codevar{ei} on line 1736 is
a high-risk looped branch to skip leading zeros, so that each window starts on a
set bit. The second branch on line 1739 runs after a full window has been
processed and the subsequent bits are zero. This is again a similar
implementation of the sliding-window exponentiation in Libgcrypt. Based on the
value of the private key exponent, a certain sequence of square operations
happens and once a complete window has been processed, the multiply operation
occurs. This was resolved differently than libgcrypt, mbedTLS used additional
montgomery reductions alongside data blinding\cite{schindler2000timing}, but even that was later
discovered to be vulnerable\cite{walter2001distinguishing}.

\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
while( 1 )
{
   ...
    ei = (E->p[nblimbs] >> bufsize) & 1;

    /*
      * skip leading 0s
      */
    <@\textcolor{red}{if( ei == 0 \&\& state == 0 )}@>
        continue;

    <@\textcolor{red}{if( ei == 0 \&\& state == 1 )}@>
    {
        /*
          * out of window, square X
          */
        MBEDTLS_MPI_CHK( mpi_montmul( X, X, N, mm, &T ) );
        continue;
    }

    /*
      * add ei to current window
      */
    state = 2;

    nbits++;
    wbits |= ( ei << ( wsize - nbits ) );

    if( nbits == wsize )
    {
        for( i = 0; i < wsize; i++ )
            MBEDTLS_MPI_CHK( mpi_montmul( X, X, N, mm, &T ) );

        MBEDTLS_MPI_CHK( mpi_montmul( X, &W[wbits], N, mm, &T ) );

        state--;
        nbits = 0;
        wbits = 0;
    }
}
\end{lstlisting}
\rulebelow
\caption{mbedTLS 2.9.0 - bignum.c lines 1717-1773 (abbreviated)}
\label{mbedtls:squareonzero}
\end{figure}

\begin{figure}
  \begin{lstlisting}
    /*
     * process the remaining bits
     */
    for( i = 0; i < nbits; i++ )
    {
        MBEDTLS_MPI_CHK( mpi_montmul( X, X, N, mm, &T ) );

        wbits <<= 1;

        <@\textcolor{red}{if( ( wbits \& ( one << wsize ) ) != 0 )}@>
            MBEDTLS_MPI_CHK( mpi_montmul( X, &W[1], N, mm, &T ) );
    }
  \end{lstlisting}
  \caption{mbedTLS 2.9.0 - bignum lines 1775-1786}
  \label{mbedtls:wbitshighrisk}
\end{figure}

\subsubsection{BearSSL 0.5}
BearSSL had no positives when setting the exponent and modulus within the
modular exponentiation code to be the tainted values. The library claims to have
constant-time implementations\cite{BearSSLweb}. The mod\_pow function itself had 0 positives,
since there were no secret dependent branches. The library contains very small
files so additional files were linked to get source which closer resembled the
source analyzed in the other libraries. The added source files included the
definitions of the functions called in the mod\_pow function.

A total of 1 positive was found with the additional source code. The branch is
found in the conversion into Montgomery form. The positive is shown on line 6 of
figure \ref{code:bear-tmont}.

\begin{figure}
  \begin{lstlisting}
void
br_i32_to_monty(uint32_t *x, const uint32_t *m)
{
  uint32_t k;

  <@\textcolor{red}{for (k = (m[0] + 31) >> 5; k > 0; k --)}@> {
    br_i32_muladd_small(x, 0, m);
  }
}
  \end{lstlisting}
  \caption{BearSSL 0.5 - br\_i32\_tmont.c}
  \label{code:bear-tmont}
\end{figure}

In this library, no structure represents the big integers, instead an array
serves the purpose. The first element in the array is used to compute the size
and all subsequent elements in the array are the data. The low number of results
here is not due to the field sensitivity, but the ability to set the tainted
variables by function context. The only variables set to be tainted in the
analysis are \codevar{e} and \codevar{m} within the \codefn{br\_i32\_modpow}
function.

The main loop, shown in figure \ref{bearssl:mainloop}, still uses the same square and multiply type method, but
BearSSL uses the square-and-always-multiply method with a constant time
conditional copy to avoid the problem.

\begin{figure}[!htpb]
  \begin{lstlisting}
    for (k = 0; k < ((uint32_t)elen << 3); k ++) {
      uint32_t ctl;

      ctl = (e[elen - 1 - (k >> 3)] >> (k & 7)) & 1;
      br_i32_montymul(t2, x, t1, m, m0i);
      CCOPY(ctl, x, t2, mlen);
      br_i32_montymul(t2, t1, t1, m, m0i);
      memcpy(t1, t2, mlen);
    }
\end{lstlisting}
\caption{BearSSL 0.5 - br\_i32\_modpow.c main loop}
\label{bearssl:mainloop}
\end{figure}


\subsubsection{OpenSSL1.1.0g}

     The number of positives was least when utilizing the field-sensitive
     analysis with a whitelist and the full big number library source. More than
     half the original number of positives were eliminated when using the best
     configuration. There were still remaining false positives, which are a
     result of pointers to different data-types pointing to the same node. In
     this library, the exponent, the plain-text and the modulus and result
     pointers all were represented by the same node. This means that even though
     in practice those pointers point to different memory locations, all were
     treated as the same memory location. The nodes were separate at the
     beginning of the analysis, but the points-to analysis deemed these nodes as
     equivalent, and merged them together in the memory graph.

     There are 5 methods for carrying out the modular exponentiation defined in
     the OpenSSL. There is the reciprocal method, and 3 variations of the
     Montgomery method, and then a simple sliding-window method. The last method
     is unreachable from the \codefn{BN\_mod\_exp} function, so no results are included for
     that method.


\noindent
\textbf{Reciprocal Method}

The reciprocal method has two looped high-risk positives both leaking data
  from the exponent used in the modular exponentiation. The variables are the
  exponent \codevar{p} and the modulus \codevar{m}. Both of these structures have
  the data elements are located as the first element in the structure. The tainted
  variables for this function are \codevar{p} 0 and \codevar{m} 0. The main
  processing loop, shown in figure \ref{code:bnexp-recp}, traverses the bits of
  \codevar{p} and looks for the first set bit, resulting in the first looped
  positive. The function \codefn{BN\_is\_bit\_set} is from \codefile{bn\_lib.c}
  and is linked with \codefile{bn\_exp.c} for the source definition. This means
  that since \codevar{p} 0 is tainted, the \codefn{BN\_is\_bit\_set} function
  result is derived from \codevar{p} 0. From the function definition in figure
  \ref{code:bn-isbitset}, the data array from the bignum structure is indexed
  using some transformation of the input parameter \codevar{n} and then masks all
  but one bit. This function leaks 1 bit of \codevar{p} at a time, but is used
  within a loop, where the input parameter is \codevar{wstart} and changes with
  the loop iteration causing more bits of \codevar{p} to be leaked as the loop
  executes.

  This result is still reported if the \codefile{bn\_lib} source file is not linked with
  the \codefile{bn\_exp.c} file. When the function definition is unreachable, all reachable
  sources from the arguments flow to the return values and other mutable
  values. In this case this means that since \codevar{p} 0 is tainted, the return value
  from the \codefn{BN\_is\_bit\_set} function is also tainted.

\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
for (;;) {
    <@\textcolor{red}{if (BN\_is\_bit\_set(p, wstart) == 0)}@> {
        if (!start)
            if (!BN_mod_mul_reciprocal(r, r, r, &recp, ctx))
                goto err;
        if (wstart == 0)
            break;
        wstart--;
        continue;
    }
   ...
}
\end{lstlisting}
\rulebelow
\caption{OpenSSL 1.1.0g - bn\_exp.c lines 250 - 259}
\label{code:bnexp-recp}
\end{figure}



\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
int BN_is_bit_set(const BIGNUM *a, int n)
{
    int i, j;

    bn_check_top(a);
    if (n < 0)
        return 0;
    i = n / BN_BITS2;
    j = n % BN_BITS2;
    if (a->top <= i)
        return 0;
    return (int)(((a->d[i]) >> j) & ((BN_ULONG)1));
}
\end{lstlisting}
\rulebelow
\caption{OpenSSL 1.1.0g - bn\_lib.c lines 741 - 753}
\label{code:bn-isbitset}
\end{figure}

Within the same processing loop, the second high-risk looped result can be
found, line 6 in figure\ref{code:bn-recp-hi2}. The analysis reports this result
for the same reason as the previous result. The analysis results are only
tracking explicit flow, but an important implicit flow result can be followed
from this positive. Since the loop variable \codevar{i} flows to the window end
variable \codevar{wend} when this branch condition is true, there is implicit
flow between the data in \codevar{p} and the \codevar{wend} variable. The
subsequent for loop then ranges from 0 to \codevar{j} a value computed simply
from \codevar{wend}. The number of iterations for the loop from 0 to \codevar{j}
is then a function of the value of \codevar{p}. In previous versions of OpenSSL,
a cache-based attack identified by observing misses that occurred
\cite{percival2005cache}. The misses allowed the attacker to know whether the
\codefn{mod\_mul} call was executing from line 18 or line 21. Though this code is
reachable, the function should not run if the constant time options are selected
for the exponent and modulus. Though this analysis is done on explicit flow, an
important implicit flow problem exists with flow from line 6 to
\codevar{wvalue}. In the montgomery constant time implementation, a defensive
scatter gather technique is used to mitigate cache-attacks that identify which
cache block was accessed by the index \codefn{wvalue}
\cite{cryptoeprint:2011:239,DBLP:journals/corr/DoychevK16}.


\begin{figure}[h!tpb]
\ruleabove
\begin{lstlisting}
for(;;){
  ...
        for (i = 1; i < window; i++) {
            if (wstart - i < 0)
                break;
            <@\textcolor{red}{if (BN\_is\_bit\_set(p, wstart - i))}@> {
                wvalue <<= (i - wend);
                wvalue |= 1;
                wend = i;
            }
        }

        /* wend is the size of the current window */
        j = wend + 1;
        /* add the 'bytes above' */
        if (!start)
            for (i = 0; i < j; i++) {
                if (!BN_mod_mul_reciprocal(r, r, r, &recp, ctx))
                    goto err;
            }
        if (!BN_mod_mul_reciprocal(r, r, val[wvalue >> 1], &recp, ctx))
            goto err;
   ...
}
\end{lstlisting}
\rulebelow
\caption{OpenSSL 1.1.0g - bn\_exp.c lines 250-297}
\label{code:bn-recp-hi2}
\end{figure}

\noindent
\textbf{Montgomery Method}

   In the high-risk classifications, there is the category of controllable
   branches. These are when there is a branch which the outcome is dependent
   both on user-input as well as secret data. As with the reciprocal method, the
   exponent and modulus, \codevar{p} 0 and \codevar{m} 0 respectively, are tainted. The
   untrusted user specified data is \codevar{a} 0 which is another bignum structure.

   Line 1 in figure \ref{code:mont-nonconst-compare} is controllable, because
   there is a comparison operation done between the plain-text and modulus.
   Assuming \codevar{a} 0 is controllable, then it could be possible to change
   \codevar{a} 0 enough to discover \codevar{m} 0. This branch is done to
   satisfy the assumption that $aa < m$ for the rest of the processing.


\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
<@\textcolor{red}{if (a->neg || BN\_ucmp(a, m) >= 0)}@> {
    if (!BN_nnmod(val[0], a, m, ctx))
        goto err;
    aa = val[0];
} else
    aa = a;

...
if (!BN_to_montgomery(val[0], aa, mont, ctx))
    goto err;               /* 1 */
\end{lstlisting}
\rulebelow
\caption{OpenSSL 1.1.0g - bn\_exp.c lines 363-368}
\label{code:mont-nonconst-compare}
\end{figure}

Line 2 of figure \ref{code:mont-nonconst-compare} calls a function that is not
linked with the analyzed source. One of the results reported from the analysis
is line 374 of \codefile{bn\_exp.c}. This line is reported due to flow from
\codevar{m} to \codevar{val[0]} from the undefined function \codefn{BN\_nnmod}.
Depending on the branch taken the value of \codevar{aa} is equivalent to
\codevar{val[0]} and thus the branch on the \codefn{BN\_to\_montgomery} call is
tainted due to the result of that branch being dependent on tainted data.

The two high-risk looped results are from code identical to the reciprocal
method branching on set bits in p within the main processing loop. These lines
for the Montgomery method are located at 416 and 437 respectively in
\codefile{bn\_exp.c}.

\noindent
\textbf{Constant-Time Montgomery Method}

There is one controllable positive for the constant-time Montgomery method. This
branch is similar to the one found in the Montgomery method, but instead of
calling \codefn{bn\_nnmod} the normal \codefn{bn\_mod} is called. This result is
seen in figure \ref{code:mont-const-compare}. The length of time to execute these
branches is longer for the case where \codevar{a} is greater than 0 and
\codevar{m}.

\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
<@\textcolor{red}{if (a->neg || BN\_ucmp(a, m) >= 0)}@> {
    if (!BN_mod(&am, a, m, ctx))
        goto err;
    if (!BN_to_montgomery(&am, &am, mont, ctx))
        goto err;
} else if (!BN_to_montgomery(&am, a, mont, ctx))
    goto err;
\end{lstlisting}
\rulebelow
\caption{OpenSSL 1.1.0g - bn\_exp.c lines 752-758}
\label{code:mont-const-compare}
\end{figure}

There are still function calls to \codefn{BN\_is\_bit\_set}, but not as branch
conditions. Instead the set bits of the window are extracted, and passed as a
parameter to select the correct value from the pre-computed table of powers.


\begin{figure}[h!]
\ruleabove
\begin{lstlisting}
for (wvalue = 0, i = bits % 5; i >= 0; i--, bits--)
    wvalue = (wvalue << 1) + BN_is_bit_set(p, bits);
bn_gather5(tmp.d, top, powerbuf, wvalue);
\end{lstlisting}
\rulebelow
\caption{OpenSSL 1.1.0g - bn\_exp.c lines 852-854}
\label{code:mont-const-scatter-gather}
\end{figure}

It is possible that there are true positives within the source code given in
figure \ref{code:mont-const-scatter-gather}. This code defines the
\codefn{bn\_gather5} function, but that source was not analyzed. The only
analyzed source files were \codefile{bn\_exp.c} and \codefile{bn\_lib.c}.

\noindent
\textbf{Montgomery Word Method}


Within the Montgomery word method, 1 high-risk looped branch is reported. The
  high-risk looped branch is the same branch from the other methods involving the
  BN\_is\_bit\_set function on the exponent \codevar{p}. This positive is found
  on line 1 of figure \ref{code:mont-word-is-bit-set}.

  \begin{figure}
    \begin{lstlisting}
<@\textcolor{red}{if (BN\_is\_bit\_set(p, b))}@> {
    next_w = w * a;
    if ((next_w / a) != w) { /* overflow */
        if (r_is_one) {
            if (!BN_TO_MONTGOMERY_WORD(r, w, mont))
                goto err;
            r_is_one = 0;
        } else {
            if (!BN_MOD_MUL_WORD(r, w, m))
                goto err;
        }
        next_w = a;
    }
    w = next_w;
}
    \end{lstlisting}
    \caption{OpenSSL 1.1.0g - bn\_exp.c lines 1200-1214}
    \label{code:mont-word-is-bit-set}
  \end{figure}


  \noindent
  \textbf{Simple Method}

  The sliding window attacks from the other libraries were also exploitable
  in earlier version of OpenSSL. The code for the function \codefn{bn\_mod\_exp\_simple} is
  specified as an entry point, although it is not reachable from the
  main entry point of the algorithm. Line 7 in figure \ref{openssl:simple} is reported
  because the branch is dependent on the current bits value of the exponent
  \codevar{p}. This result is directly computed from the exponent and within a
  loop so the amount of leakage is high if the branch outcome is known. The
  timing-channels within this implementation was found in the timing difference
  between the \codefn{mod\_mul} where \codevar{r} is squared
  \cite{percival2005cache}. The implementation of the \codefn{mod\_mul}
  function calls \codefn{bn\_sqr} if both inputs are the same, otherwise the
  more expensive \codefn{bn\_mul} is called.


     \begin{figure}[!htpb]
       \begin{lstlisting}
        j = wstart;
        wvalue = 1;
        wend = 0;
        for (i = 1; i < window; i++) {
            if (wstart - i < 0)
                break;
            <@\textcolor{red}{if (BN\_is\_bit\_set(p, wstart - i))}@> {
                wvalue <<= (i - wend);
                wvalue |= 1;
                wend = i;
            }
        }

        /* wend is the size of the current window */
        j = wend + 1;
        /* add the 'bytes above' */
        if (!start)
            for (i = 0; i < j; i++) {
                if (!BN_mod_mul(r, r, r, m, ctx))
                    goto err;
            }

        /* wvalue will be an odd number < 2^window */
        if (!BN_mod_mul(r, r, val[wvalue >> 1], m, ctx))
            goto err;
       \end{lstlisting}
       \caption{OpenSSL 1.1.0g - bn\_exp.c lines 1326 - 1350}
       \label{openssl:simple}
     \end{figure}

\subsection{Verifying a 2-safety Property}

\begin{table*}[!t]
  \centering
    \ra{1.2}
  \begin{tabular}{@{}lcrrrrcrrrr@{}}
    \toprule
     & & \multicolumn{4}{c}{Ct-verif} & &  \multicolumn{4}{c}{CtChecker} \\
    \cmidrule{3-6} \cmidrule{8-11}
    Library&& Baseline & No Loops & Removed & Diff && Baseline & No Loops & Removed & Diff\\
    \midrule
    Libgcrypt 1.8.2                 			&&  45  & 25 & 0 & 20 &&	 	25 & 25 & 1 & 0 \\
    BearSSL 0.5                     			&&  14  & 10 & 0 &  4  && 		10 & 10 & 0 & 0 \\
    mbedTLS 2.9.0                  		 	&& 100 & 80 & 0 & 20 && 		77 & 77 & 1 & 0 \\
    OpenSSL 1.1.0g                                                                                 \\
    \hspace{0.25cm}Reciprocal       		&&  23  & 19 & 0 &  4   && 	19 & 19 & 0 & 0 \\
    \hspace{0.25cm}Mont.            		&&  29  & 23 & 0 &  6   && 	21 & 21 & 0 & 0 \\
    \hspace{0.25cm}Mont Const. Time 	&&  39  & 28 & 0 & 11  && 	25 & 25 & 2 & 0 \\
    \hspace{0.25cm}Mont. Word       		&&  16  & 15 & 0 &  1   && 	15 & 15 & 0 & 0 \\
    \bottomrule
  \end{tabular}
\caption{ The baseline file (version 2) accommodated the excluded source.  Positives caused by loops were removed from the baseline in a separate file (version 3). A new file removed the remaining positives (version 4).}
\label{tbl:runtimes}
\end{table*}

In this section, we discuss another approach to verify constant time code,
and compare the precision and accuracy between it and CtChecker. We consider
a recent effort that verifies a 2-property, expressing the security of a
program as a set of logical statements or conditions, and verifying them
automatically using a theorem solver. Our goal was to perform a reasonable
comparison that focuses on the advantages of each method, and not the
engineering details associated with implementing them.

\subsubsection{Background}
The paper of Almeida et al. uses an approach based on a reduction of the
security of a program \textit{P} to the assertion-safety of a program
\textit{Q}, and implements it in a prototype, Ct-verif.
\cite{almeida2016verifying}. The reduction is inspired by prior work
on self-composition and product programs.  A product program \textit{Q}
verifies the security of \textit{P} by simulating two executions in lockstep.
It asserts the equality of each public input with its renamed copy at each
branching instruction. Public outputs are not taken into consideration, and
the technique constructs an output-insensitive product program.  Theoretically,
the approach is sound and complete in that all safe programs have a correct
security verdict, and all unsafe programs have a correct insecurity verdict.

Ct-verif verifies optimized LLVM by implementing their reduction
technique.  The SMACK verification tool is used as a front-end to compile the
annotated C-code via Clang and the resulting compiled and optimized LLVM code
is translated to Boogie code.  The reduction is performed on the Boogie code and
then applied to the Boogie verifier, which uses an SMT logic solver.  Ct-verif
provides an annotation interface for public inputs.

The implementation does not provide results for the entire source. During its
product program verification, if an error state is reached (a violation is
found), no transition is enabled and the system will stop its execution. Constant
time violations that occurred first in sequence were reported, but
identification of violations that occurred afterwards was not guaranteed.
To identify all violations, we replaced each error with some constant time
code and re-ran the tool on the revised code.

We perform a reasonable comparison between the two tools to better understand
the distinct advantages of each method. We aim to avoid the drawbacks of the tools
caused by engineering issues; we assume the best setting for each tool.

\subsubsection{Limitations of Ct-verif}
Although the approach is theoretically sound and complete, the results of
a practical interpretation, must be carefully analyzed.  To perform
a reasonable comparison, we avoid key limitations not present in CtChecker by
revising the analyzed source to accommodate Ct-verif's technical flaws.  For
instance, loops and non-static length arrays are not supported.

Loop invariants are automatically computed to verify program loops.  When
counters are used to index arrays, a proof of security would require Ct-verif
to infer that the memory accesses are in the range of public values.  \sout{Although
there are options to avoid this difficulty, such as enforcing a loop limit or
unrolling a loop, the [actually, why doesn't this work?].}  To avoid this
limitation, loops were replaced with a branch statement that depended on the
values that affected the loop bounds.

Constant sized arrays are supported by adding an extra annotation, but variable
sized arrays are not. To ensure memory accesses are not in the range of secret
values, array indexing needs to be fixed.  We dictated that any array access
was fixed in between these annotated bounds.  False positives caused by loops
or variable array accesses were included in a separate result for Ct-verif. The
revisions made to replace a for-loop and a variable array access in \codefn{MPN\_NORMALIZE}
are shown in figure 30.

We did not include the full source in all of the libraries, except for in BearSSL,
which has a small amount of reachable code from the modular exponentiation function.
Undefined values, such as those that are assigned to the return value of a function
and are excluded from the analysis, were replaced with new variables.  CtChecker handles
excluded source code by conservatively tainting the return value of a function
whose parameter is tainted.  In contrast, Ct-verif taints the return value
regardless of the state of the parameters.  In this direct comparison, we replaced
the excluded function's return value with some new variable.  If the return value
was part of a branch condition, and the function was called with tainted
parameters, this line was counted as a positive for both tools.
In figure 28, \codefn{gcrympih\_lshift} is not linked in the analysis, so its
return value is unknown.  Thus, \codevar{carry\_limb} is undefined before the
branch, and the verifier throws an error.  The revision occurs on line 2 wherein
\codevar{carry\_limb} is given a reserved public input.

\begin{figure}[h!]
\begin{lstlisting}
carry_limb = _gcry_mpih_lshift( res->d, rp, rsize, mod_shift_cnt);
carry_limb = PUBLIC_VALUE;
rp = res->d;
if ( carry_limb )
{
    rp[rsize] = carry_limb;
    rsize++;
}
\end{lstlisting}
\caption{Libgcrypt 1.8.2 - mpi-pow.c lines 717-723}
\label{code:libgcrypt_normalize}
\end{figure}

\subsubsection{Results}
We note that all true positives were reported among both tools, so we
observed comparable accuracy.  Mainly, the results explain a couple of
reasons for the significant difference between the precision observed.
Several versions of the  cryptography algorithms were created before
comparing the precision of the two tools against one another.  There
are three versions (version 2, version 3 and version 4, where version
1 is the original file), each of which was developed from the version
number before it.

First, a baseline version was created to accommodate minor limitations
or differences between the tools, such as the way in which they operate
with some undefined values. Any assignment statement with excluded
source was rewritten to assign the variable to a public or private
input, depending on the function return value. If any of the functions
parameters were tainted, we conservatively tainted its return value.
Thus, a function call inside of a branch condition was counted as a
positive when any of its parameters were tainted. These positives were
counted the same for each tool, and were only observed in OpenSSL.

The second version was constructed from the first, and it allowed us to
examine the number of positives introduced from computing loop invariants.
Each erroneous loop was replaced, and each variable array access was
changed to a fixed one. Loops were replaced by a model that leaked the
same information as the loop: a \codevar{if (e) then} statement where
\codevar{e} contains the loop bounds. Figure 30 is an example of two
revisions in Libgcrypt, one for a loop and one for an array access.
Ct-verif supports an annotation for a static number of values to be
marked as public.  Variable length arrays are not supported, so variable
array element accesses needed to be removed. Every array index was fixed
within the bounds of the array.

The difference in the number of positives reported by Ct-verif in the
baseline and no loops versions is the number of positives caused by
computing loop invariants only.  The number is a significant portion
of the total positives reported on each version. As expected, CtChecker reported the
same number of positives after removing loops.

The third version was created from the second to remove each erroneous
line reported by Ct-verif, resulting in a subset of the original file
that was free of any constant time violation. This final version allowed
us to evaluate the precision of CtChecker in another way; any positive
reported on this version was exclusive to CtChecker, and was expected
to be a false one.

Ct-verif is flow sensitive; the order of statements in a program may
affect the analysis.  CtChecker's flow insensitivity is one cause of
false positives in the results.

Version 4 was constructed by replacing non-constant time code to
intentionally observe 0 positives by Ct-verif. The positives reported
by CtChecker in this final version was the result two design choices
distinct from Ct-verif. First, CtChecker is a flow insensitive analysis
(the order of statements in a program does not matter). In figure 29,
variable \codevar{i} is not tainted until line 851, but flow
insensitivity causes CtChecker to flag line 1041 since \codevar{i} is
assigned to a tainted value, \codevar{bits}. Second, rather than
tainting a range of values, CtChecker taints the variable pointing to
them. In figure 32, \codevar{ep} is a pointer to sensitive data.

Overall, CtChecker exhibited a large improvement precision over
Ct-verif, a result apparent in the difference between the number of
positives reported by each tool in version 2. CtChecker showed equal
or improved precision over Ct-verif when disregarding significant
limitations, such as loop invariants. This result is apparent in the
difference between the number of positives reported by each tool in
version 3.

\begin{figure}[h!]
\begin{lstlisting}
tmp.d[0] = (0 - m->d[0]) & BN_MASK2;
<@\textcolor{red}{for (i = 1; i < top; i++)}@>
    tmp.d[i] = (~m->d[i]) & BN_MASK2;
tmp.top = top;
...
for (wvalue = 0, i = bits % window; i >= 0; i--, bits--)
\end{lstlisting}
\caption{OpenSSL 1.1.0g - bn\_exp.c lines 741-1041}
\label{code:libgcrypt_normalize}
\end{figure}

\subsubsection{Conclusion}
Perhaps the simple heuristic used for-loop invariant generation is the
primary reason the 2-property analysis saw a lesser precision.  Additionally,
while deductive verification is sufficient for verifying non-trivial programs, like
the ones in this paper, it remains difficult to use. As previously mentioned,
there is no trivial method to interpret verification failures. CtChecker uses
many fewer annotations to run its analysis, and provides a complete list of
violations for each source line.

\begin{figure}[h!]
\begin{lstlisting}
#define MPN_NORMALIZE(d, n)    \
    do {		                   \
      if((n) > 0) /*while( (n) > 0 )*/ { \
        if((d)[0]) /*if( (d)[(n)-1] )*/ \
            dummy++; //break;	 \
        (n)--;	               \
      }		                     \
} while(0)
\end{lstlisting}
\caption{Libgcrypt 1.8.2 - mpi-internal.h lines 113-120}
\label{code:libgcrypt_normalize}
\end{figure}


\begin{figure}[h!]
\begin{lstlisting}
<@\textcolor{red}{if ( rp == ep )}@>
{
    /* RES and EXPO are identical.  Allocate temp. space for EXPO.  */
    ep_nlimbs = esec? esize:0;
    ep = ep_marker = mpi_alloc_limb_space( esize, esec );
    MPN_COPY(ep, rp, esize);
}
\end{lstlisting}
\caption{mbedTLS 2.9.0 - bignum.c lines 1778-1786}
\label{code:libgcrypt_normalize}
\end{figure}


\begin{table*}[!t]
  \centering
    \ra{1.2}
  \begin{tabular}{@{}lcrrrrcrrrr@{}}
    \toprule
    Library && Baseline & No Loops & Removed \\
    \midrule
    Libgcrypt 1.8.2                	    	 && 23 & 18 & 0 \\
    BearSSL 0.5                     	    	 && 16 & 3 & 0 \\
    mbedTLS 2.9.0                          	 && 84 & 67 & 0 \\
    OpenSSL 1.1.0g                                                                                 \\
    \hspace{0.25cm}Reciprocal       	 && 14 & 14 & 0 \\
    \hspace{0.25cm}Mont.            	 && 19 & 16 & 0 \\
    \hspace{0.25cm}Mont Const. Time && 34 & 25 & 0 \\
    \hspace{0.25cm}Mont. Word       	 && 16 & 15 & 0 \\
    \bottomrule
  \end{tabular}
\caption{ }
\label{tbl:runtimes}
\end{table*}
