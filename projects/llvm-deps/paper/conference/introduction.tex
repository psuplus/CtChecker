\section{Introduction}

Timing channels can be found in modern cryptographic systems that are widely
adopted \cite{kochertiming,brumley2005remote,percival2005cache,bernstein2017sliding}.
These widely adopted systems are used to provide confidentiality and integrity
of data communicated between parties. Attackers can exploit these timing
channels to compromise the assumptions of these safe communication methods.
Timing channels exist in many forms and are often the result of implementation
details which are not available in theoretical proposals of a protocol.

In cryptosystems, an effective countermeasure against timing attacks is the
constant-time programming discipline.  While a strict enforcement of this
discipline will likely rule out timing attacks, doing so is also infeasible for
a couple of reasons: (1) identifying non-constant time code  is often an
error-prone and manual process, (2) real-world code typically contains
intentional non-constant fragments that does not leak crucial information.

Many methods have emerged as a response to attempt to automate the process of
identifying timing channels. Currently as vulnerabilities are
identified, the measures taken to mitigate these vulnerabilities are highly
specific to the problem. It is important to be able to analyze software to
identify possible timing channels, and mitigate them as necessary. The problem
becomes identifying the vulnerabilities. Static analyses exist to be able to
identify these vulnerabilities.
  
  Static analyses can struggle with the number of false positives being fairly
  high. The precision is often sacrificed to decrease the runtime of an
  analysis. Practical applications of a static analysis would optimally have the
  least amount of false positives while remaining sound. Developers and
  engineers who design and implement these libraries can then more easily focus
  on the implementations most likely to be vulnerable.
  
  There are many types of static analysis targeted at identifying timing
  channels \cite{cached-zhang, brotzmancasym, molnar2005program}. There have also been
  works which analyze cryptographic protocols using proof-engineering techniques
  \cite{proof-engineering}. The issue with these approaches is in terms of
  scalability and accurate representation of system state. Static analysis tools
  have the ability to work directly with the binary so the state is closer to
  the actual state during execution of the software.

  This work is based on a static analysis which tracked information-flow in
  order to find variables computed from secret values as well as find
  secret-dependent branches \cite{moore2011static}. The analysis uses
  \textit{DSA}, a points-to analysis to handle dynamically allocated memory
  \cite{DSA-lattner}. The information-flow analysis is heavily reliant on the
  points-to analysis to track the information flow of practical programs. The
  precision of the information-flow analysis is influenced by how it integrates
  with the points-to analysis.
  
  The static information flow analysis looks at the effects of adding extra
  features to the baseline information-flow analysis to increase precision. The
  positives that are eliminated should not be any high-risk positives and the
  total number should be fewer than the baseline. In the case study, the
  analysis is run on modular exponentiation source code used in popular
  cryptosystems, with additional features. There were two features added to the
  baseline analysis to try and achieve this goal, i) field-sensitivity, ii)
  white-listing. Lastly, the analysis has to be conservative when source code
  for a given function call is not provided, so the analysis was tested with and
  without the full library source code.
  
  The field sensitivity is improved by relying on more information provided by
  the points-to analysis. The white-list is aimed at removing results that may
  actually be leaking information but are considered acceptable. The white-list
  can also be used to ignore results which cause other erroneous positives. 

  A case-study is performed by analyzing the modular exponentiation source code
  provided for Libgcrypt, OpenSSL, mbedTLS, and BearSSL. The results help
  identify what features are necessary to improve the precision of an
  information-flow analysis. For example, in Libgcrypt the number of results was
  reduced from 64 to 24 total results. Of the 64 baseline results there were 5
  classified as high-risk and in the improved analysis the high-risk results
  were still present. The reduced number of results came from eliminated false
  positives.


