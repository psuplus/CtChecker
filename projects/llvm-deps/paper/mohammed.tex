\documentclass[11pt,a4paper]{article}

%\usepackage[margin=0.75in]{geometry}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}
\lstset{numbers=left, numberstyle=\tiny, breaklines}
\title{CSE 597 Homework 1\vspace{-1ex}}
\author{Adam Mohammed}
\date{\vspace{-1ex}January 29, 2018}

\begin{document}
\section{Introduction}
%
\section{Background}
\subsection{Information Flow Analysis}
  Information flow analysis is a tool to track the interactions of information
  throughout the program. Information flow has been used in compilers as well as
  security to gain knowledge about the structure of the program. The structure
  can be exploited by compilers to improve cache locality, hide memory latency
  and other performance tweaks. In security the idea of information flow has
  been used in things like Mandatory Access Control schemes such as LOMAC and
  the Biba model. For program level security, a compiler could be able to check
  the level of integrity for any variable through the use of information flow.
  By tracking confidential or sensitive pieces of information, any variable or
  decision which is computed from a confidential data has a flow
  from the confidential data to the variable or branch. An attacker may be able
  to exploit parts of a program which are dependent on some confidential data,
  and with sufficient flow, can possibly reconstruct the confidential data. A
  toy example is given in Algorithm \ref{alg:simpleflow} to illustrate this concern.

  \begin{algorithm}
    \caption{Simple Information Flow}
    \label{alg:simpleflow}
  \begin{algorithmic}
    \State $k =$ sensitive information
    \State $d.x =  k + 1$
    \State $d.y = 0$
    \State $a = -1$

    \If {$d.y == k/2$}
      \State $a = 4$ 
    \EndIf

    % \If {$d.x == 2$}
      % \State $b = 2$ 
    % \Endif
  \end{algorithmic}
  \end{algorithm}

  In this simple example $k$ is confidential data, through information flow we
  can see that the value of $d.x$ is directly computed from $k$. The value $d.y$
  however is not confidential due to its value having no reliance on the
  confidential data. The flow between $k$ and $d.x$ is called explicit flow,
  since the value of $d.x$ is computed from the confidential data. As the
  analysis continues, any value which is computed from $d.x$ will also be
  treated as confidential data. There is also implicit flow which can be seen in
  the first branch on $d.y$ where it is compared with some value computed from
  $k$. In this case d.y is still considered non-confidential data, since the
  value has no relation to $k$, but there is a flow from $k$ to $a$ indirectly.
  Although, $a$ is not computed from $k$, after the branch executes, the value
  of $a$ may or may not have changed. If it does change, the value of k has been
  determined. If it does not change, the value of k is not determined, but a
  possible value has been ruled out. This indirect flow from $k$ to $a$ is what
  is called implicit flow.
\subsection{Points-to Analysis}

For an accurate sensitive data tracking analysis it is essential to know what
portions of memory are being manipulated. This can be a trivial issue if
assumptions such that all variables are constant and no modifications are made,
but for most programming paradigms this assumption does not hold. If it is
possible for a variable to point to locations in memory, tracking the flow of
sensitive data depends on where the pointers lead. The need for a analysis to
keep track of the data which is pointed to by each pointer. This analysis is
necessary to understand the program state at any given time. The points-to
analysis represents the data as nodes in a graph. Each node represents one data
structure and each edge pointing to that node is a pointer by which the node can
be reached. Each of these nodes provides a conservative representation of the
type information for the data structure it represents. The type information
available from each node is only available if the points-to analysis determines
the representation is unable to be modified during the run-time. In the case
where the type information is able to be modified during run-time the points-to
analysis returns no type information for the node. In the improved sensitive
data tracking analysis, when pointers are encountered the target of the pointer
is resolved using the points-to analysis.

\subsection{Timing Channels in Crypto-systems}

\section{Improving Field Sensitivity in Taint Analysis}
In this work, improvements were made over a baseline taint analysis. The
original taint analysis will be referred to as the baseline analysis and the
improved analysis will be referred to as the improved analysis. The changes
made from the baseline to the improved analysis were done to improve the
field sensitivity. The system works by generating constraints for each value
that is extracted from the control flow of a program. By creating the con-
straint pairs, tracking tainted values and their effect on the rest of the pro-
gram becomes a problem of tracking the least upper bound for each value.
A real example from AES is taken to show how the sensitivity is improved.

\begin{algorithm}
  \caption{Public and private data in structure}
  \label{alg:aes_struclt}
  \lstinputlisting{examples/aes.c}
\end{algorithm}

  In this work, improvements were made over a baseline taint analysis. The
  original taint  
  The changes made since the previous versions of the pass were done to improve
  the sensitivity. The system works by generating constraints for each value that
  is extracted from the control flow of a program. By creating the constraint
  pairs, tracking tainted values and their effect on  the rest of the program
  becomes a problem of tracking the least upper bound for each value.  Originally,
  there was a mapping between values and AbstractLocs which are the data structure
  used to represent the organization of memory. For user-defined types, such as
  classes and structures, one AbstractLoc represented the memory created from
  defining an object from one of the types. consequently, as the original design
  worked just one constraint element mapped to the AbstractLoc despite one
  AbstractLoc representing more than one data field. As a result, if any part of
  the AbstractLoc had a least upper bound that was the same as the tainted
  variable all related fields were considered tainted.

  For this work, changes were made to a compile-time analysis to improve
  the precision. The analysis is built to help the developer find and identify
  places where secret information may leak to an adversary. By tracking the
  information flow during the optimization stage of compiling, the analysis can
  reduce the work the developer needs to do to ensure information integrity is
  maintained. The analysis to provide this level of assurance relies on being
  able to model the memory and accesses to it in conjunction with the source
  and control-flow graph representations of the program. The granularity
  that the analysis can provide can change the number of reported results
  considerably. Before the changes the granularity of the analysis was done
  by object (or structure), meaning if an object at any point had information
  flow from a sensitive piece of information, all data in that object would
  now be considered sensitive. The major change addressed in this work is to
  improve the granularity of the analysis. The motivation for this change is
  to reduce the number of positives that result only as a consequence of the
  poor granularity.


  Algorithm \ref{alg:simpleflow}, let the variable $k$ be a sensitive variable in the following program. Also, let
  variable $d$ be a structure which contains two fields $x$ and $y$. 

  \begin{algorithm}
    \caption{Simple Information Flow}
    \label{alg:simpleflow}
  \begin{algorithmic}
    \State $k =$ sensitive information
    \State $d.x =  k + 1$
    \State $d.y = 4$
    %\IF {$d.y == 2$}
      %\STATE $a = key$[0] 
    %\ENDIF
    % \If {$d.x == 2$}
      % \State $b = 2$ 
    % \Endif
  \end{algorithmic}
  \end{algorithm}

  \begin{algorithm}

  \end{algorithm}

  At the start, the $k$ variable holds sensitive information and any flow that has k
  in its computation should be sensitive. In line 2, $k$ influences the value of $d.x$
  and as expected $d.x$ is sensitive. Ideally $d.x$ is the only variable other
  than $k$ that is reported from the analysis. In the original implementation,
  the compiler uses a memory representation that represents the memory that would
  be associated with $d$ and because of the object-level granularity, instead of
  $d.x$ being the only variable that is sensitive from $d$, $d.y$ is also listed
  as a sensitive variable. This simple case results yields a very conservative
  bound on what variables of the program have actually been reached by a sensitive
  piece of information. The draw back from being this conservative is that many of
  the positives that are reported are just a result of being in the same memory
  structure element as the actual variable which has been computed from a
  sensitive piece of information.

  In terms of correctness, $d.y$ has actually never been computed from any
  sensitive information and thus should not be a tainted variable. False
  positives like this propagate throughout the program and for larger programs
  account for a significant amount of the results. The fix to this issue
  requires there to be work done with how constraints are created from elements
  which share the same data type and the same memory element that represented
  that variable in the analysis. First, the issue is to understand how the
  analysis as it is done in the original work functions, and then the necessary
  changes can be addressed.

  The analysis runs as one of the middle layers of compilation. The program
  traverses through and decodes the operands from each instruction that
  generates the flow of information between variables in the program.

  Establishing constraints is done by traversing through the IR and creating a
  set of inequalities between the the source and sink values for each
  instruction. Mathematically, if information flows from a source to a sink, the
  sink must be considered at least as sensitive as the components of which it is
  computed. Thus any information that is considered sensitive is given a high
  starting sensitivity value to force any information it touches to also raise
  the sensitivity value of any variable which is computed from it. Once the set
  of constraints are created and solved, a variable is considered sensitive if
  its sensitivity value is at least that of the value assigned to sensitive
  information.

  
  For this to function as expected, just looking at the operands in the IR is
  not enough. If information flows from pointer operands, the points-to analysis
  is recruited to more precisely track the true source and sink data operands.
  Pointers and the target data are represented in a graph in the points-to
  analysis. Any operand which is a pointer, should have a node in the points-to
  graph and then the node itself can be used to help identify and properly
  constrain the data elements that are used in the computation of each variable.

  In the baseline analysis, each time a operand operated from a base address
  there was one memory element which represented the data from that pointer. If
  any field which is based from that pointer was computed from a sensitive
  value, all fields represented by that pointer would be considered sensitive.
  This is where the major modification needed to be made. Instead of one memory
  element representing all fields from a pointer, each pointer would need to map
  to be able to be treated independently.

\begin{algorithm}
\lstinputlisting{examples/struct_ex.c}
\caption{Structure Inaccuracy}
\label{alg:inacc}
\end{algorithm}

  Improving field sensitivity helps in reducing the number of false positives.
  Without augmentation the analysis would over-estimate the number of affected
  values for any given confidential information flow. In this example a
  structure type is defined such that it has two fields contained within it,
  both integers. The only variable that is set to be confidential is
  \texttt{key}. Lines 6,7,8 are all just declarations, at this point no
  variables are confidential apart from \texttt{key}. Line 10, the field of
  \texttt{a} in variable \texttt{u} is set to be the value of \texttt{key}. Here
  this means that there is a flow of information from the \texttt{key} variable
  to the \texttt{a} variable in the \texttt{u} data structure. Line 11, though
  \texttt{b} resides in the same data structure, \texttt{u}, as \texttt{a},
  there has been no flow from between \texttt{key} and \texttt{b}. At this point
  the confidential variables are \texttt{key} and $u.a$. As for the branches,
  line 13 is decided based on confidential data but line 15 is not. Line 14 is
  considered if implicit flow is considered, because the state of \texttt{ct}
  change based on the value of \texttt{key}.

\begin{algorithm}
\lstinputlisting{examples/minimal_struct_ex.ll}
\caption{LLVM intermediate representation}
\label{alg:ab_ir}
\end{algorithm}


The intermediate representation that is analyzed is similar to the code shown in
Algorithm \ref{alg:ab_ir}. This is not the exact representation from LLVM, but
this does display the vital components that the improved analysis leverages to
increase sensitivity. This snippet shows the IR code to accomplish lines 10 and
11 from Algorithm \ref{alg:inacc}. First, the value of key is loaded into a
register \texttt{\%0}, creating a flow from the value of key to the register
\texttt{\%0}. Next there is an instruction which gets the address of field
\texttt{a} from the structure \texttt{u}, which is located at an offset of 0.
Finally, line 3 completes storing the value of key to the location pointed to by
\texttt{\%a}. The taint analysis then needs a way to figure out what
\texttt{\%a} actually points to, to properly track the flow. Since \texttt{u} is
essentially a base address from which the offsets for fields \texttt{a} and
\texttt{b} are calculated the points to analysis keeps track of that memory
location. When an operation occurs in that region of memory, like when we do a
store to \texttt{u.a} there the points to analysis is used to track the memory location
that is modified. In this case it is trivial because the base pointer never
changes, but in cases where the base pointer may be modified, the points to
analysis maintains accurate representation of memory modifications. With the
aid of the points-to analysis, the taint analysis can track what parts of
memory were computed from confidential data. The next constraint that is
generated is the flow from \texttt{\%0} to the memory location which represents
\texttt{u} and ideally should only make \texttt{a} confidential. In the original
analysis, all structures were treated as one entity and flow from a confidential
source to a memory location was less precise. Precision in this context meant
that even though \texttt{a} and \texttt{b} are computed from the same base pointer and memory
location, only \texttt{a} has been affected by the confidential value. The original
analysis would instead have two values which are confidential \texttt{u.a} and \texttt{u.b}
due to this precision issue. As a result, the number of false positives are
lower with the analysis with increased precision. For algorithm \ref{alg:inacc},
lines 10, 11, 13 and 15 have potentially been computed from the confidential
value \texttt{key}, when the more precise analysis reports lines 10 and 13 as lines
which are dependent on the confidential value. Lines 11 and 15 are reported due
to \texttt{u.b} and \texttt{u.a} both adding constraints on the same memory structure, but not
distinguishing which part of the structure. The way to address this issue is to
consider the offset from the base pointer, as seen in lines 2 and 4 of Algorithm
\ref{alg:ab_ir}, given by the operands \texttt{i31 0} and \texttt{i32 1}.


  

% The desired change would allow each field in a
% user-defined type to be treated independently. The major change made to allow
% this type of behavior was to change how constraints were generated for
% AbstractLocs. Instead of one constraint which represented the entire memory
% location, constraints could be generated based on the offset accessed during a
% particular instruction. GetElementPtrInst (GEP instructions) are the type of
% instructions that are used to access a particular offset in to a pointer type
% in LLVM. The analysis generates constraints by moving through the flow
% information of the program. When iterating through the flow of information,
% some changes were made to handle these GEP instructions values differently
% than other value types.

%#
%  Any time a GEP instruction was captured, code was added to calculate what
%  offset was being accessed. If this was the first time an instruction tried to
%  access that memory as represented by an AbstractLoc, constraint elements were
%  generated based on the type information for that node in memory. If it had
%  already been accessed, the constraint element corresponding to the offset from
%  the instruction was returned. Once the constraint element is identified, the
%  right and left hand constraint elements are joined to add a constraint rule to
%  the programs set of constraints. This allowed individual fields from within
%  the same memory node could be tracked independently and could potentially
%  reduce the number of false positives compared to the previous design. There
%  are some challenges with allowing many constraints elements per memory node.
%
%  First, the GEP instruction value itself in LLVM represents the offset as an
%  index, not a byte offset. Meaning that if two separate GEP instruction uses
%  and offset of 2, one could index 8 bytes into the memory and the other 2
%  bytes. So in the new design this is handled by inspecting the LLVM value in
%  conjunction with the type information from the memory node to find the correct
%  element.
%
%  This leads to the second issue, the memory representation used in the analysis
%  can occasionally not be accompanied by type information. In this case, the
%  design defaults to using the LLVM representation of whatever type is being
%  addressed, and assumes field widths and that data elements do not overlap.
% The last change was made to the files that set the tainted values in either the
%
% vulnerable branch pass or the tainted analysis pass. These files read the lines
% from two files to establish what is to be tainted and untrusted. The values are
% set to be tainted by iterating through the list of values for the program. If a
% type which is mapped to a memory location is to be tainted there is a function
% to get the offset for that piece of memory, and consequently the constraint
% element tied to it. Previously, these offsets were calculated by going to the
% memory node type information, but changes were made here to properly get the
% correct offset when dealing with GEP instructions.maketitle
\section{Evaluation}
\subsection{Implementation}

Achieving the improved precision of the analysis, is done primarily through
handling LLVM's pointer instruction differently than other instructions. The
pointer instruction in the IR is the GetElementPtr instruction. This instruction
is used when computing an address from a base pointer. For arrays, for example
it has the index of the element and the bounds of the array if they are known.
For structures, the instruction contains the structure type which is being
addressed and the index of the field in the structure that is being referenced.
The baseline analysis lacked the ability to consider the index available in
these instructions. The points-to analysis had type and offset
information for the targets of the pointers. Improving the analysis was achieved
by i) creating the appropriate number of constraint elements for each node in
the points-to graph, ii) converting the field index from the GEP instruction to
a byte offset, and iii) constraining the correct elements based on
the GEP instruction and data type information.

\subsubsection{Constraint Element Generation}
When a pointer value is encountered for the first time, a node is created for it
in the points-to graph, and a set of constraint elements are created for that
data structure. The points-to analysis was left unmodified, so the focus will be
on the semantics of how constraint elements were changed. In the baseline, only
one constraint element represented any target of a pointer. In the improved
analysis, type information from the points-to analysis was used to generate the
appropriate number of constraint elements. In a stack allocated array, one
constraint element was created for each element in the array. For structures and
classes, each field is located at some offset away from the base address of the
structure and a constraint element was created for each field and mapped to
associated byte offset. This change enables the precision of the baseline to be
improved by correctly selecting the corresponding constraint element based on
the IR.

For any GEP instruction, constraint elements are generated using the type
information from the points-to analysis when available. Each constraint element
is created by iterating through the type information of the node from the
points-to analysis. For each type there is an associated byte offset, using this
offset and the size as reported by LLVM, a constraint element is created with
the corresponding starting byte offset and the ending byte offset based on the
width of the field. It is necessary to account for the width and start location
of each field because there may be padding between fields of a structure.
Padding between fields may be present in the type information because of an
unused field in code. The points-to analysis will not have type information on
unused fields so it is important to account for these gaps. Another reason to
account for the padding is due to structures which are not aligned with each
field following sequentially after the previous field. When the type information
is not available one constraint element is created for the whole node so at
worst the improved analysis will be equivalent to that of the baseline.

Algorithm \ref{alg:generate_conselem} shows how the points-to analysis is
leveraged to create constraint elements for the proper byte offsets and widths.
Let \texttt{s} be the LLVM representation of the structure type referenced by
the GEP instruction, and \texttt{node} be the node in the memory graph provided
by the points-to analysis. The algorithm works by retrieving the graph in which
the node resides and then retrieving the data layout as specified by the
compiler for that graph. The data layout contains information like the field
lengths of each type and the alignment within a data structure. The structure
type itself \texttt{s} does not have the alignment and padding information, only
the types of each field within the data structure. The data layout is used to
retrieve the structure layout of the structure type from the GEP instruction.
The structure layout is vital because it allows the index from the GEP
instruction to be converted to a byte offset within the data structure. The
data layout also provides the size of each type, so that the constraint element
is created at the corrects starting and ending offset.
\begin{algorithm}
  \caption{Creating constraint elements for each field in a type}
  \label{alg:generate_conselem}
  \lstinputlisting{examples/createConsElem.cpp}
\end{algorithm}

The elements created using this strategy enable the analysis to be more precise.
GEP instructions can also be used to index arrays, so that is handled by
computing the number of elements that array may hold. For stack arrays that
number is known, but for heap arrays the number is variable. For stack arrays
each, since the number of elements is known, one constraint element is created
per array element. For heap arrays, one constraint element is created for the
entire array.

Similarly, if type information is unavailable then a conservative approach is
used. One constraint element is created for the entire data structure. When
the information is available, the analysis is able to be improved by
constraining the correct constraint element for each instruction. When that
information is unavailable, the improved analysis is unable to be more precise
than the baseline analysis.

\subsubsection{Constraining Operands}
Each time an instruction where information flow exists, constraints are
generated between the operands of that instruction. The idea of this is simple,
given a set of constraint elements select the ones which are used in the
operation and constrain them as necessary. The baseline analysis handled
individual variables in a precise manner as long as the variable was not a
structure or array. The baseline analysis had no way of identifying the offset
from a GEP instruction, and even if it did, the mapping between a memory node to
a constraint element was one-to-one. In the previous section, the mapping from
memory node to constraint element was made to be one-to-many. As one node may
represent multiple fields which together make the structure or array. It is now
necessary to pick the correct element from this one-to-many mapping when
generating the constraint for each instruction. Again, this is done by analyzing
the GEP instruction.

The last operand of a GEP instruction is the index of the field within the data
structure. Each constraint element for a memory node is created for a byte
offset range, so it is necessary to correctly calculate the byte offset from the
field index provided by the GEP instruction. This is very similar to the
strategy used to generate the constraints at the correct byte locations. Given
the structure type, field index and node from the points-to analysis, the offset
is calculated as shown in Algorithm \ref{alg:find_offset}. Given the node, the
data layout and associated structure layout can be found and then the index can
be converted to a byte offset.

\begin{algorithm}
  \caption{Calculating byte offset from index}
  \label{alg:find_offset}
  \lstinputlisting{examples/findoffset.cpp}
\end{algorithm}

If the GEP instruction is in reference to an array access, then the offset is
just calculated using the size of the element type and the index. If no type
information exists for the node, only a single constraint element would exist
for the node, so that is the constraint which is selected to be constrained. 

\subsection{Benchmarks}
% list of the tests

\subsection{Case Study}

\section{Related Works}

\section{Conclusion}

\end{document}
