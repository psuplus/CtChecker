\documentclass[11pt,a4paper]{article}

%\usepackage[margin=0.75in]{geometry}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}
\lstset{numbers=left, numberstyle=\tiny, breaklines}
\title{CSE 597 Homework 1\vspace{-1ex}}
\author{Adam Mohammed}
\date{\vspace{-1ex}January 29, 2018}

\begin{document}
\section{Introduction}
%
\section{Background}
\subsection{Information Flow Analysis}
  Information flow analysis is a tool to track the interactions of information
  throughout the program. Information flow has been used in compilers as well as
  security to gain knowledge about the structure of the program. The structure
  can be exploited by compilers to improve cache locality, hide memory latency
  and other performance tweaks. In security the idea of information flow has
  been used in things like Mandatory Access Control schemes such as LOMAC and
  the Biba model. For program level security, a compiler could be able to check
  the level of integrity for any variable through the use of information flow.
  By tracking confidential or sensitive pieces of information, any variable or
  decision which is computed from a confidential data has a flow
  from the confidential data to the variable or branch. An attacker may be able
  to exploit parts of a program which are dependent on some confidential data,
  and with sufficient flow, can possibly reconstruct the confidential data. A
  toy example is given in Algorithm \ref{alg:simpleflow} to illustrate this concern.

  \begin{algorithm}
    \caption{Simple Information Flow}
    \label{alg:simpleflow}
  \begin{algorithmic}
    \State $k =$ sensitive information
    \State $d.x =  k + 1$
    \State $d.y = 0$
    \State $a = -1$

    \If {$d.y == k/2$}
      \State $a = 4$ 
    \EndIf

    % \If {$d.x == 2$}
      % \State $b = 2$ 
    % \Endif
  \end{algorithmic}
  \end{algorithm}

  In this simple example $k$ is confidential data, through information flow we
  can see that the value of $d.x$ is directly computed from $k$. The value $d.y$
  however is not confidential due to its value having no reliance on the
  confidential data. The flow between $k$ and $d.x$ is called explicit flow,
  since the value of $d.x$ is computed from the confidential data. As the
  analysis continues, any value which is computed from $d.x$ will also be
  treated as confidential data. There is also implicit flow which can be seen in
  the first branch on $d.y$ where it is compared with some value computed from
  $k$. In this case d.y is still considered non-confidential data, since the
  value has no relation to $k$, but there is a flow from $k$ to $a$ indirectly.
  Although, $a$ is not computed from $k$, after the branch executes, the value
  of $a$ may or may not have changed. If it does change, the value of k has been
  determined. If it does not change, the value of k is not determined, but a
  possible value has been ruled out. This indirect flow from $k$ to $a$ is what
  is called implicit flow.
\subsection{Field Sensitivity Points to Analysis}
  For an accurate taint tracking analysis it is essential to know what portions of
  memory are being manipulated. This can be a trivial issue if assumptions such
  that all variables are constant and no modifications are made, but for most
  programming paradigms this assumption does not hold. If variables can be just
  pointers to locations in memory, there needs to be a way to figure out what the
  pointer is able to address to be able to accurately track the state of the
  program at any given time. The static analysis tool upon which this improvement
  was made utilized a points to analysis to track the memory locations pointed to
  and modified by pointers. Each time the analysis encountered a pointer type the
  tool enlisted the help of the memory model provided by the points to analysis.
  The content of a pointer and the address of a pointer can be treated as two
  separate values and though related, can be tainted individually. 
\subsection{Goal: Timing Channels in Crypto-systems}

\section{Improving Field Sensitivity in Taint Analysis}

  The changes made since the previous versions of the pass were done to improve
  the sensitivity. The system works by generating constraints for each value that
  is extracted from the control flow of a program. By creating the constraint
  pairs, tracking tainted values and their effect on  the rest of the program
  becomes a problem of tracking the least upper bound for each value.  Originally,
  there was a mapping between values and AbstractLocs which are the data structure
  used to represent the organization of memory. For user-defined types, such as
  classes and structures, one AbstractLoc represented the memory created from
  defining an object from one of the types. consequently, as the original design
  worked just one constraint element mapped to the AbstractLoc despite one
  AbstractLoc representing more than one data field. As a result, if any part of
  the AbstractLoc had a least upper bound that was the same as the tainted
  variable all related fields were considered tainted.

  For this work, changes were made to a compile-time analysis tool to improve the
  accuracy. The tool is built to help the developer find and identify places where
  secret information may leak to an adversary. By tracking the information flow
  during the optimization stage of compiling, the tool can reduce the work the
  developer needs to do to ensure information integrity is maintained. The
  analysis to provide this level of assurance relies on being able to model the
  memory and accesses to it in conjunction with the source and control-flow graph
  representations of the program. The granularity that the analysis can provide
  can change the number of reported results considerably. Before the changes the
  granularity of the analysis was done by object (or structure), meaning if an
  object at any point had information flow from a sensitive piece of information,
  all data in that object would now be considered sensitive. The major change
  addressed in this work is to improve the granularity of the analysis. The
  motivation for this change is to reduce the number of positives that result only
  as a consequence of the poor granularity.

  Let the variable $k$ be a sensitive variable in the following program. Also, let
  variable $d$ be a structure which contains two fields $x$ and $y$.

  \begin{algorithm}
    \caption{Simple Information Flow}
  \begin{algorithmic}
    \State $k =$ sensitive information
    \State $d.x =  k + 1$
    \State $d.y = 4$
    %\IF {$d.y == 2$}
      %\STATE $a = key$[0] 
    %\ENDIF
    % \If {$d.x == 2$}
      % \State $b = 2$ 
    % \Endif
  \end{algorithmic}
  \end{algorithm}

  \begin{algorithm}

  \end{algorithm}

  At the start, the $k$ variable holds sensitive information and any flow that has k
  in its computation should be sensitive. In line 2, $k$ influences the value of $d.x$
  and as expected $d.x$ is sensitive. Ideally $d.x$ is the only variable other
  than $k$ that is reported from the analysis. In the original implementation,
  the compiler uses a memory representation that represents the memory that would
  be associated with $d$ and because of the object-level granularity, instead of
  $d.x$ being the only variable that is sensitive from $d$, $d.y$ is also listed
  as a sensitive variable. This simple case results yields a very conservative
  bound on what variables of the program have actually been reached by a sensitive
  piece of information. The draw back from being this conservative is that many of
  the positives that are reported are just a result of being in the same memory
  structure element as the actual variable which has been computed from a
  sensitive piece of information.


  In terms of correctness, $d.y$ has actually never been computed from any
  sensitive information and thus should not be a tainted variable. False
  positives like this propagate throughout the program and for larger programs
  account for a significant amount of the results. The fix to this issue
  requires there to be work done with how constraints are created from elements
  which share the same data type and the same memory element that represented
  that variable in the analysis. First, the issue is to understand how the
  analysis as it is done in the original work functions, and then the necessary
  changes can be addressed.

  The analysis runs as one of the middle layers of compilation. The program
  traverses through and decodes the operands from each instruction that
  generates the flow of information between variables in the program.

  The way we set up our constraints to find which values are sensitive, is by
  traversing through the IR and creating an inequality between the operands used
  to compute the value, or sources, and the operand which corresponds to the
  destination for that value, or the sink. Each constrained is listed in a way
  such that the source must be less than or equal to the sink. There are 4 main
  scenarios, neither operand is sensitive, only the source is sensitive, only
  the sink is sensitive, and finally both operands are sensitive prior to the
  instruction.

  In the first case, it is trivial, if neither operands are sensitive, the
  result is a non-sensitive value. In the case of the source being sensitive
  prior to the operation, but not the sink, the result is that both the source
  and sink are sensitive following the operation. In the reverse case, the
  source is non-sensitive and the sink is sensitive, there is no change after
  this operation. The sink is still sensitive and the source has gained no
  information about the sink, so it is still non-sensitive.

  By solving a set of constraints mathematically, the values which have become
  sensitive will all have a sensitive tag. Often, flow of information is not
  clear just from the operands, which is why a memory model is also used to
  accurately track information flow. The main issue that requires the memory
  when pointer math is used to access an element, such as array accesses or
  class fields accesses. So it is essential to know where each element in a
  structure or array may lie when evaluating these instructions for information
  flow.

  As it worked before, each time a operand operated from a base address
  there was one memory element which represented the data from that pointer. If
  any field which is based from that pointer was computed from a sensitive
  value, all fields represented by that pointer would be considered sensitive.
  This is where the major modification needed to be made. Instead of one memory
  element representing all fields from a pointer, each pointer would need to map
  to be able to be treated independently.

  Improving field sensitivity helps in reducing the number of false positives.
  Without augmentation the analysis would over-estimate the number of affected
  values for any given confidential information flow. In this example a
  structure type is defined such that it has two fields contained within it,
  both integers. The only variable that is set to be confidential is
  \texttt{key}. Lines 6,7,8 are all just declarations, at this point no
  variables are confidential apart from \texttt{key}. Line 10, the field of
  \texttt{a} in variable \texttt{u} is set to be the value of \texttt{key}. Here
  this means that there is a flow of information from the \texttt{key} variable
  to the \texttt{a} variable in the \texttt{u} data structure. Line 11, though
  \texttt{b} resides in the same data structure, \texttt{u}, as \texttt{a},
  there has been no flow from between \texttt{key} and \texttt{b}. At this point
  the confidential variables are \texttt{key} and $u.a$. As for the branches,
  line 13 is decided based on confidential data but line 15 is not. Line 14 is
  considered if implicit flow is considered, because the state of \texttt{ct}
  change based on the value of \texttt{key}.

\begin{algorithm}
\lstinputlisting{examples/struct_ex.c}
\caption{Structure Inaccuracy}
\label{alg:inacc}
\end{algorithm}

The intermediate representation that is analyzed is similar to the code shown in
Algorithm \ref{alg:ab_ir}. This is not the exact representation from LLVM, but
this does display the vital components that the improved analysis leverages to
increase sensitivity. This snippet shows the IR code to accomplish lines 10 and
11 from Algorithm \ref{alg:inacc}. First, the value of key is loaded into a
register \texttt{\%0}, creating a flow from the value of key to the register
\texttt{\%0}. Next there is an instruction which gets the address of field
\texttt{a} from the structure \texttt{u}, which is located at an offset of 0.
Finally, line 3 completes storing the value of key to the location pointed to by
\texttt{\%a}. The taint analysis then needs a way to figure out what
\texttt{\%a} actually points to, to properly track the flow. Since \texttt{u} is
essentially a base address from which the offsets for fields \texttt{a} and
\texttt{b} are calculated the points to analysis keeps track of that memory
location. When an operation occurs in that region of memory, like when we do a
store to \texttt{u.a} there the points to analysis is used to track the memory location
that is modified. In this case it is trivial because the base pointer never
changes, but in cases where the base pointer may be modified, the points to
analysis maintains accurate representation of memory modifications. With the
aid of the points-to analysis, the taint analysis can track what parts of
memory were computed from confidential data. The next constraint that is
generated is the flow from \texttt{\%0} to the memory location which represents
\texttt{u} and ideally should only make \texttt{a} confidential. In the original
analysis, all structures were treated as one entity and flow from a confidential
source to a memory location was less precise. Precision in this context meant
that even though \texttt{a} and \texttt{b} are computed from the same base pointer and memory
location, only \texttt{a} has been affected by the confidential value. The original
analysis would instead have two values which are confidential \texttt{u.a} and \texttt{u.b}
due to this precision issue. As a result, the number of false positives are
lower with the analysis with increased precision. For algorithm \ref{alg:inacc},
lines 10, 11, 13 and 15 have potentially been computed from the confidential
value \texttt{key}, when the more precise analysis reports lines 10 and 13 as lines
which are dependent on the confidential value. Lines 11 and 15 are reported due
to \texttt{u.b} and \texttt{u.a} both adding constraints on the same memory structure, but not
distinguishing which part of the structure. The way to address this issue is to
consider the offset from the base pointer, as seen in lines 2 and 4 of Algorithm
\ref{alg:ab_ir}, given by the operands \texttt{i31 0} and \texttt{i32 1}.


  

\begin{algorithm}
\lstinputlisting{examples/minimal_struct_ex.ll}
\caption{LLVM intermediate representation}
\label{alg:ab_ir}
\end{algorithm}

% The desired change would allow each field in a
% user-defined type to be treated independently. The major change made to allow
% this type of behavior was to change how constraints were generated for
% AbstractLocs. Instead of one constraint which represented the entire memory
% location, constraints could be generated based on the offset accessed during a
% particular instruction. GetElementPtrInst (GEP instructions) are the type of
% instructions that are used to access a particular offset in to a pointer type
% in LLVM. The analysis generates constraints by moving through the flow
% information of the program. When iterating through the flow of information,
% some changes were made to handle these GEP instructions values differently
% than other value types.

%#
%  Any time a GEP instruction was captured, code was added to calculate what
%  offset was being accessed. If this was the first time an instruction tried to
%  access that memory as represented by an AbstractLoc, constraint elements were
%  generated based on the type information for that node in memory. If it had
%  already been accessed, the constraint element corresponding to the offset from
%  the instruction was returned. Once the constraint element is identified, the
%  right and left hand constraint elements are joined to add a constraint rule to
%  the programs set of constraints. This allowed individual fields from within
%  the same memory node could be tracked independently and could potentially
%  reduce the number of false positives compared to the previous design. There
%  are some challenges with allowing many constraints elements per memory node.
%
%  First, the GEP instruction value itself in LLVM represents the offset as an
%  index, not a byte offset. Meaning that if two separate GEP instruction uses
%  and offset of 2, one could index 8 bytes into the memory and the other 2
%  bytes. So in the new design this is handled by inspecting the LLVM value in
%  conjunction with the type information from the memory node to find the correct
%  element.
%
%  This leads to the second issue, the memory representation used in the analysis
%  can occasionally not be accompanied by type information. In this case, the
%  design defaults to using the LLVM representation of whatever type is being
%  addressed, and assumes field widths and that data elements do not overlap.
% The last change was made to the files that set the tainted values in either the
%
% vulnerable branch pass or the tainted analysis pass. These files read the lines
% from two files to establish what is to be tainted and untrusted. The values are
% set to be tainted by iterating through the list of values for the program. If a
% type which is mapped to a memory location is to be tainted there is a function
% to get the offset for that piece of memory, and consequently the constraint
% element tied to it. Previously, these offsets were calculated by going to the
% memory node type information, but changes were made here to properly get the
% correct offset when dealing with GEP instructions.maketitle
\section{Evaluation}
\subsection{Implementation}
%
\subsection{Benchmarks}
% list of the tests

\subsection{Case Study}

\section{Related Works}

\section{Conclusion}

\end{document}
